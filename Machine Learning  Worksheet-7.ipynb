{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc3b66d",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef4868",
   "metadata": {},
   "source": [
    "                                                       Assignment -7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e4f52",
   "metadata": {},
   "source": [
    "1. Which of the following in sk-learn library is used for hyper parameter tuning?\n",
    "A) GridSearchCV() B) RandomizedCV()\n",
    "C) K-fold Cross Validation D) All of the above\n",
    "\n",
    "Ans: D) All of the above.\n",
    "\n",
    "\n",
    "2. In which of the below ensemble techniques trees are trained in parallel?\n",
    "A) Random forest B) Adaboost\n",
    "C) Gradient Boosting D) All of the above\n",
    "\n",
    "Ans: A) Random forest.\n",
    "\n",
    "3. In machine learning, if in the below line of code:\n",
    " sklearn.svm.SVC (C=1.0, kernel='rbf', degree=3)\n",
    "we increasing the C hyper parameter, what will happen?\n",
    "A) The regularization will increase B) The regularization will decrease\n",
    "C) No effect on regularization D) kernel will be changed to linear \n",
    "\n",
    "Ans: A) The regularization will increase \n",
    "\n",
    "4. Check the below line of code and answer the following questions:\n",
    "sklearn.tree.DecisionTreeClassifier(*criterion='gini',splitter='best',max_depth=None,\n",
    "min_samples_split=2)\n",
    "Which of the following is true regarding max_depth hyper parameter?\n",
    "A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.\n",
    "B) It denotes the number of children a node can have.\n",
    "C) both A & B\n",
    "D) None of the above\n",
    "\n",
    "Ans:A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.\n",
    "\n",
    "\n",
    "5. Which of the following is true regarding Random Forests?\n",
    "A) It's an ensemble of weak learners.\n",
    "B) The component trees are trained in series\n",
    "C) In case of classification problem, the prediction is made by taking mode of the class labels \n",
    "predicted by the component trees.\n",
    "D)None of the above\n",
    "\n",
    "Ans:A) It's an ensemble of weak learners.\n",
    "\n",
    "6. What can be the disadvantage if the learning rate is very high in gradient descent?\n",
    "A) Gradient Descent algorithm can diverge from the optimal solution.\n",
    "B) Gradient Descent algorithm can keep oscillating around the optimal solution and may not settle.\n",
    "C) Both of them\n",
    "D) None of them\n",
    "\n",
    "Ans:A) Gradient Descent algorithm can diverge from the optimal solution.\n",
    "\n",
    "7. As the model complexity increases, what will happen?\n",
    "A) Bias will increase, Variance decrease B) Bias will decrease, Variance increase\n",
    "C)both bias and variance increase D) Both bias and variance decrease.\n",
    "\n",
    "Ans:B) Bias will decrease, Variance increase\n",
    "\n",
    "8. Suppose I have a linear regression model which is performing as follows:\n",
    " Train accuracy=0.95 and Test accuracy=0.75\n",
    "Which of the following is true regarding the model?\n",
    "A) model is underfitting B) model is overfitting\n",
    "C) model is performing good D) None of the above\n",
    "\n",
    "Ans:B) model is overfitting.\n",
    "\n",
    "**Q9 to Q15 are subjective answer type questions, Answer them briefly.**\n",
    "\n",
    "9. Suppose we have a dataset which have two classes A and B. The percentage of class A is 40% and \n",
    "percentage of class B is 60%. Calculate the Gini index and entropy of the dataset.\n",
    "\n",
    "Ans:     Proportion of class A: 0.4\n",
    "         Proportion of class B: 0.6\n",
    "         Gini = 1 - (p_A^2 + p_B^2)\n",
    "         Gini = 1 - (0.4^2 + 0.6^2) = 1 - (0.16 + 0.36) = 1 - 0.52 = 0.48\n",
    "         \n",
    "        Therefore, the Gini index of the dataset is 0.48.\n",
    "        Entropy = - (p_A log2 p_A + p_B log2 p_B)\n",
    "Entropy = - (0.4 log2 0.4 + 0.6 log2 0.6) = - (-0.5288 - 0.4422) = -(-0.9710) = 0.9710\n",
    "\n",
    "Therefore, the entropy of the dataset is 0.9710.        \n",
    "        \n",
    "\n",
    "10. What are the advantages of Random Forests over Decision Tree?\n",
    "\n",
    "Ans: Random Forests have several advantages over Decision Trees:\n",
    "\n",
    "Reduced overfitting: Decision Trees have a tendency to overfit the training data, resulting in poor generalization to unseen data. Random Forests address this problem by using multiple trees and aggregating their predictions, which reduces the risk of overfitting.\n",
    "\n",
    "Improved accuracy: Random Forests can achieve higher accuracy than Decision Trees by using multiple trees and aggregating their predictions. This helps to reduce the impact of noisy data and outliers, which can be a problem for Decision Trees.\n",
    "\n",
    "Robustness to outliers: Decision Trees can be sensitive to outliers in the data, which can affect the split criteria and lead to suboptimal trees. Random Forests address this problem by using multiple trees and aggregating their predictions, which reduces the impact of outliers.\n",
    "\n",
    "Feature selection: Random Forests can be used for feature selection by examining the importance of each feature in the classification task. This can help to identify the most relevant features and reduce the dimensionality of the problem.\n",
    "\n",
    "Scalability: Random Forests can be trained on large datasets in parallel, which makes them scalable and suitable for big data problems. In contrast, Decision Trees are not easily scalable and can become computationally expensive for large datasets.\n",
    "\n",
    "Overall, Random Forests are a more robust and accurate machine learning algorithm compared to Decision Trees, especially in complex classification tasks with high-dimensional data and noisy features.\n",
    "\n",
    "11. What is the need of scaling all numerical features in a dataset? Name any two techniques used for \n",
    "scaling.\n",
    "\n",
    "Ans:Scaling all numerical features in a dataset is necessary because it helps to normalize the features and bring them to a common scale, which can improve the performance of many machine learning algorithms. Here are some reasons why scaling is important:\n",
    "\n",
    "Different features may have different units or scales, which can bias the algorithm towards certain features that have larger values or variances.\n",
    "\n",
    "Some algorithms, such as k-NN, SVM, and K-Means, are sensitive to the scale of the features and may produce suboptimal results if the features are not scaled.\n",
    "\n",
    "Scaling can also help to speed up the optimization process during model training by reducing the range of values that need to be searched.\n",
    "\n",
    "Here are two common techniques used for scaling:\n",
    "\n",
    "Min-Max scaling: This technique scales the data to a fixed range of values, typically between 0 and 1. The formula for min-max scaling is:\n",
    "\n",
    "x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "where x is the original value, x_min is the minimum value of the feature, and x_max is the maximum value of the feature.\n",
    "\n",
    "Standardization: This technique scales the data to have zero mean and unit variance. The formula for standardization is:\n",
    "\n",
    "x_scaled = (x - mean) / std_dev\n",
    "\n",
    "where x is the original value, mean is the mean value of the feature, and std_dev is the standard deviation of the feature.\n",
    "\n",
    "Both of these techniques can help to normalize the features and improve the performance of machine learning algorithms. The choice of scaling technique may depend on the specific requirements of the problem and the characteristics of the data.\n",
    "\n",
    " **MACHINE LEARNING**\n",
    " \n",
    "**ASSIGNMENT - 7**\n",
    "\n",
    "\n",
    "12. Write down some advantages which scaling provides in optimization using gradient descent \n",
    "algorithm.\n",
    "\n",
    "Ans:Scaling can provide several advantages in optimization using gradient descent algorithm:\n",
    "\n",
    "Faster convergence: Scaling can help the gradient descent algorithm converge faster by reducing the oscillations and making the optimization path smoother. This is because scaling makes the contour lines of the cost function more circular, which allows the gradient descent algorithm to move more efficiently towards the minimum.\n",
    "\n",
    "Prevents overflow and underflow: Scaling can help prevent overflow and underflow errors when computing the gradients, especially for large or small input values. This is because scaling reduces the range of the input values and brings them to a common scale, which can improve numerical stability.\n",
    "\n",
    "Better conditioning: Scaling can improve the condition number of the Hessian matrix, which measures the curvature of the cost function. This can help the gradient descent algorithm converge more quickly and accurately by reducing the sensitivity to the step size.\n",
    "\n",
    "Enables the use of adaptive learning rates: Scaling can enable the use of adaptive learning rates, such as Adam or Adagrad, which can adjust the step size based on the gradient magnitude. This can help the gradient descent algorithm converge more quickly and accurately by adapting to the local curvature of the cost function.\n",
    "\n",
    "Overall, scaling can improve the performance of the gradient descent algorithm by reducing oscillations, preventing numerical errors, improving conditioning, and enabling adaptive learning rates. It is an important preprocessing step for many machine learning algorithms that rely on gradient-based optimization.\n",
    "\n",
    "13. In case of a highly imbalanced dataset for a classification problem, is accuracy a good metric to \n",
    "measure the performance of the model. If not, why?\n",
    "\n",
    "Ans:In case of a highly imbalanced dataset for a classification problem, accuracy is generally not a good metric to measure the performance of the model. This is because accuracy can be misleading and give a false sense of good performance if the majority class dominates the dataset.\n",
    "\n",
    "For example, if the dataset has 90% of the samples belonging to the majority class and only 10% belonging to the minority class, a classifier that always predicts the majority class will have an accuracy of 90%. However, this classifier is essentially useless for predicting the minority class, which is the class of interest.\n",
    "\n",
    "In such cases, it is better to use metrics that are sensitive to the performance of the minority class, such as precision, recall, F1-score, or area under the ROC curve (AUC-ROC). These metrics take into account both true positives and false positives, and can provide a more balanced view of the performance of the classifier.\n",
    "\n",
    "For instance, precision measures the fraction of true positives out of all predicted positives, while recall measures the fraction of true positives out of all actual positives. F1-score is the harmonic mean of precision and recall, and is a good metric when both precision and recall are important. AUC-ROC measures the trade-off between true positive rate and false positive rate across different classification thresholds.\n",
    "\n",
    "Therefore, it is important to choose appropriate evaluation metrics that are relevant to the problem and take into account the imbalanced nature of the dataset.\n",
    "\n",
    "14. What is â€œf-score\" metric? Write its mathematical formula.\n",
    "\n",
    "Ans:F-score is a popular evaluation metric used in binary classification problems, which combines the precision and recall of the classifier into a single measure. The F-score is the harmonic mean of precision and recall, and is often used when the class distribution is imbalanced.\n",
    "\n",
    "The mathematical formula for F-score is:\n",
    "\n",
    "F-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "where precision is the fraction of true positives out of all predicted positives, and recall is the fraction of true positives out of all actual positives.\n",
    "\n",
    "Precision can be calculated as:\n",
    "\n",
    "precision = true positives / (true positives + false positives)\n",
    "\n",
    "where true positives are the number of positive instances correctly predicted by the model, and false positives are the number of negative instances wrongly predicted as positive.\n",
    "\n",
    "Recall can be calculated as:\n",
    "\n",
    "recall = true positives / (true positives + false negatives)\n",
    "\n",
    "where false negatives are the number of positive instances wrongly predicted as negative.\n",
    "\n",
    "The F-score ranges from 0 to 1, where a score of 1 indicates perfect precision and recall, while a score of 0 indicates no true positive predictions. The F-score gives equal weight to precision and recall, and is a good metric when both are important, such as in medical diagnosis or fraud detection problems.\n",
    "\n",
    "15. What is the difference between fit(), transform() and fit_transform()\n",
    "\n",
    "Ans:In machine learning, fit(), transform() and fit_transform() are methods used for data preprocessing and feature engineering.\n",
    "\n",
    "fit(): The fit() method is used to estimate the parameters (e.g., mean, variance, etc.) of a preprocessing step from the training data. These parameters are then used to transform both the training and test data using the transform() method. In other words, fit() is used to learn the internal state of the transformation based on the training data.\n",
    "\n",
    "transform(): The transform() method is used to apply a specific transformation to the input data. This method takes the input data as an argument and returns the transformed data based on the parameters learned by the fit() method. It is important to note that the transform() method does not modify the internal state of the transformation.\n",
    "\n",
    "fit_transform(): The fit_transform() method is a combination of the fit() and transform() methods. It is used to learn the parameters from the training data using the fit() method and apply the transformation to both the training and test data using the transform() method in a single step. This can be more efficient than using fit() and transform() separately.\n",
    "\n",
    "To summarize, fit() is used to estimate the parameters of the transformation, transform() is used to apply the transformation to the data, and fit_transform() is a convenience method that combines the fit() and transform() methods into a single step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96661ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
